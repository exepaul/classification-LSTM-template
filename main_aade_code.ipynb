{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "validation_data=['Legal Sparring Continues in Bitcoin User s Battle with IRS Tax Sweep  http  ift tt number iTYYDC http  ift tt number iWnLed','MISSED BITCOIN DONT MISS THIS PR REG NOW legal block chain DO NOT MISS THIS http  tcpros co number XhMx crypto blockchain money euro','you are bad','you are not good','Well that dip under $1000 lasted about three years longer than I expected','Legal Sparring Continues in Bitcoin Userâ€™s Battle with IRS Tax Sweep','dip down under water','MISSED BITCOIN DONT MISS THIS PR REG NOW legal block chain DO NOT MISS THIS http  tcpros co number XhMx crypto blockchain money euro']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_list=np.load('wordsList.npy')\n",
    "word_vector=np.load('wordVectors.npy')\n",
    "vectors=np.load('idsMatrix.npy')\n",
    "epoch=50\n",
    "batch_size=25\n",
    "iteration=int(len(vectors)//batch_size)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batch_size, 250])\n",
    "    for i in range(batch_size):\n",
    "        if (i % 2 == 0):\n",
    "            num = random.randint(1,11499)\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            num = random.randint(13499,24999)\n",
    "            labels.append(0)\n",
    "        arr[i] = vectors[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "labels_datr=[1,0]\n",
    "           # 0  1  2\n",
    "labels2index={j:i for i,j in enumerate(labels_datr)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "url='https://hooks.slack.com/services/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'\n",
    "\n",
    "\n",
    "\n",
    "def keeping_track_count(count_no,accuracy_count,loss_count,epoch_count):\n",
    "    payload = {\n",
    "    \"attachments\": [\n",
    "        {\n",
    "            \"title\": \"The Further Adventures of Slackbot\",\n",
    "\n",
    "            \"text\" : 'hello guy',\n",
    "\n",
    "\n",
    "    },{\n",
    "            \"title\": \"The Further Adventures of Slackbot\",\n",
    "\n",
    "            \"text\" : 'hello guy77',\n",
    "\n",
    "    }]}\n",
    "    payload[\"attachments\"][0][\"title\"] = \"epoch\" + \"  \"+ str(epoch_count)\n",
    "    payload[\"attachments\"][1][\"title\"] = \"iteration_no\" + \"  \"+ str(count_no)\n",
    "\n",
    "    payload[\"attachments\"][0][\"text\"] = \"loss\" + \"  \" + str(loss_count)\n",
    "    payload[\"attachments\"][1][\"text\"] = \"accuracy\" + \" \" + str(accuracy_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    r = requests.post(url, data=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMclassifier():\n",
    "    def __init__(self, hdim, lables):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # placeholders\n",
    "        input_x = tf.placeholder(tf.int32, shape=[None, None],name='input')\n",
    "        output_y = tf.placeholder(tf.int32, shape=[None,])\n",
    "\n",
    "        self.placeholder = {'input': input_x, 'output': output_y}\n",
    "\n",
    "        # word_embedding\n",
    "        word_embedd = tf.get_variable('embedding', shape=[400000, 50], dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(np.array(word_vector)), trainable=False)\n",
    "        embedding_lookup = tf.nn.embedding_lookup(word_embedd, input_x)\n",
    "\n",
    "        # sequence_length\n",
    "        sequence_le = tf.count_nonzero(input_x, axis=-1)\n",
    "\n",
    "        # model\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            cells = rnn.LSTMCell(num_units=hdim)\n",
    "            cell = rnn.DropoutWrapper(cell=cells, output_keep_prob=0.80)\n",
    "\n",
    "            model = tf.nn.bidirectional_dynamic_rnn(cell, cell, inputs=embedding_lookup, sequence_length=sequence_le,\n",
    "                                                    dtype=tf.float32)\n",
    "\n",
    "        final_output, (fs, fc) = model\n",
    "\n",
    "        # transform_output\n",
    "        final_output_forward = tf.transpose(final_output[0], [1, 0, 2])\n",
    "        final_output_backward = tf.transpose(final_output[1], [1, 0, 2])\n",
    "\n",
    "#         state_output = tf.concat([fs.c, fc.c], axis=-1)\n",
    "        final_output_both = tf.concat([final_output_forward[0], final_output_backward[0]], axis=-1)\n",
    "\n",
    "        # weights and fully_connected_layer\n",
    "        weights = tf.get_variable('weights', shape=[2 * hdim, lables], dtype=tf.float32,\n",
    "                                  initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "\n",
    "        bias = tf.get_variable('bias', shape=[lables,], dtype=tf.float32,\n",
    "                               initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "\n",
    "        # logits ( final_output_matrix )\n",
    "\n",
    "        logits_ = tf.matmul(final_output_both, weights) + bias\n",
    "\n",
    "        # normalization\n",
    "        prob = tf.nn.softmax(logits_,name='prob')\n",
    "        pred = tf.argmax(prob, axis=-1,name='predt')\n",
    "\n",
    "        # cross_entropy\n",
    "        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_, labels=output_y)\n",
    "        loss = tf.reduce_mean(ce)\n",
    "\n",
    "        # accuracy\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                (tf.equal(\n",
    "                    tf.cast(pred, tf.int32), output_y)),\n",
    "                tf.float32))\n",
    "\n",
    "        # training\n",
    "        training_ = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "        self.out = {'logits': logits_,\n",
    "                    'prob': prob,\n",
    "                    'pred': pred,\n",
    "                    'loss': loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    'train': training_\n",
    "                    }\n",
    "\n",
    "#         self.shape_checking = {'embedding_l': embedding_lookup,  # 2x4x100\n",
    "#                                'final_output': final_output,  # 2x4x10\n",
    "#                                'fs': fs,  # 2x10\n",
    "#                                'transpo': final_output_backward,  # 4x2x10\n",
    "#                                'for': final_output_forward,  # 4x2x10\n",
    "#                                'state': state_output,  # 2x20\n",
    "#                                'fina': final_output_both,  # 2x20\n",
    "#                                'weig': weights,  # 20x10\n",
    "#                                'log': logits_}  # 2x10\n",
    "\n",
    "#         self.interaction ={'prob':prob,\n",
    "#                            'pred':pred,\n",
    "#                            'logits__':logits_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         }\n",
    "\n",
    "\n",
    "# checking model with dummy data\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_exe(model):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(epoch):\n",
    "            for j in tqdm(range(iteration)):\n",
    "                # batch = vectors[j * batch_size:(j + 1) * batch_size]\n",
    "                # tweets_data = np.array(padding_data([aa for aa, bb in batch])['input'])\n",
    "                # labels_data = np.array([labels_datr.index(bb) for aa, bb in batch])\n",
    "\n",
    "                labess, tweetsr = getTrainBatch()\n",
    "\n",
    "                out_a = sess.run(model.out, feed_dict={model.placeholder['input']: labess,\n",
    "                                                       model.placeholder['output']: tweetsr})\n",
    "\n",
    "                keeping_track_count(j,out_a['accuracy'],out_a['loss'],i)\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "        saver.save(sess, '/home/ayodhyankit/sentimnt_aadi/training_data/testing_fix/fix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = LSTMclassifier(250, len(labels_datr))\n",
    "    out = rand_exe(model)\n",
    "    print(out['prob'],out['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
